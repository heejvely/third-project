{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7b05d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "관련기사 건수 57.0\n",
      "기사 2개, 선택된 url https://news.naver.com//main/clusterArticles.naver?id=c_202111071110_00000169&mode=LSD&mid=shm&sid1=101&oid=214&aid=0001161265\n",
      "제외된 언론사 조선일보 경향신문\n",
      "선택한 언론사 ['한겨레', '한국일보', '동아일보', '중앙일보']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>media</th>\n",
       "      <th>title</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021.11.21. 오후 4:54</td>\n",
       "      <td>중앙일보</td>\n",
       "      <td>대체 얼마나 나오길래…'역대급' 종부세 폭탄 날, 회견도 연다</td>\n",
       "      <td>\\t\\n\\t  올해분 종합부동산세 납세 고지서가 22일부터 날아든다. 지난해보다 크...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021.11.21. 오후 3:39</td>\n",
       "      <td>한겨레</td>\n",
       "      <td>올해 종부세 ‘세금폭탄’ 아니라 ‘이 빠진 호랑이’에 가깝다</td>\n",
       "      <td>국세청 22일 종부세 고지서 발송종부세 내일 고지한다는데…집부자는 공제·증여 ‘방어...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021.11.21. 오후 5:31</td>\n",
       "      <td>한국일보</td>\n",
       "      <td>종부세 대상자 10만명 증가, 세수는 4배 늘듯... 고지서 22일부터 발송</td>\n",
       "      <td>늘어난 세 부담에 역대급 증여 이어질 듯월세로 세금 내는 월세화 현상 가속화 전망도...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time media                                       title  \\\n",
       "0  2021.11.21. 오후 4:54  중앙일보          대체 얼마나 나오길래…'역대급' 종부세 폭탄 날, 회견도 연다   \n",
       "1  2021.11.21. 오후 3:39   한겨레           올해 종부세 ‘세금폭탄’ 아니라 ‘이 빠진 호랑이’에 가깝다   \n",
       "2  2021.11.21. 오후 5:31  한국일보  종부세 대상자 10만명 증가, 세수는 4배 늘듯... 고지서 22일부터 발송   \n",
       "\n",
       "                                            document  \n",
       "0  \\t\\n\\t  올해분 종합부동산세 납세 고지서가 22일부터 날아든다. 지난해보다 크...  \n",
       "1  국세청 22일 종부세 고지서 발송종부세 내일 고지한다는데…집부자는 공제·증여 ‘방어...  \n",
       "2  늘어난 세 부담에 역대급 증여 이어질 듯월세로 세금 내는 월세화 현상 가속화 전망도...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def main_page(url,topic):\n",
    "    # 모듈 import\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup as bs\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # 정치면 메인 페이지 요청 -> html로 파싱\n",
    "    global headers\n",
    "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\"}\n",
    "    req = requests.get(url, headers = headers)\n",
    "    target = bs(req.content, 'html.parser')\n",
    "\n",
    "    url_main = 'https://news.naver.com/'\n",
    "\n",
    "    total = {}\n",
    "    link = {}\n",
    "    if topic == '정치':\n",
    "        select = target.select('div.cluster_foot_inner > a')\n",
    "    else:\n",
    "        select = target.select('div.cluster_head_inner > a')\n",
    "        \n",
    "    for idx, tag in enumerate(select):\n",
    "\n",
    "        each_url = url_main + tag['href'] # 각 헤드라인 서브 페이지 링크\n",
    "\n",
    "        try: \n",
    "            news_num = int(re.match('[0-9]+', tag.text).group()) # 관련 기사 갯수\n",
    "        except:\n",
    "            news_num = np.nan\n",
    "\n",
    "        # 헤드라인 서브 페이지 요청 -> html로 파싱\n",
    "        sub_req = requests.get(each_url, headers = headers)\n",
    "        page = bs(sub_req.content, 'html.parser')\n",
    "\n",
    "        # 헤드라인 서브 페이지에서 가장 첫번째 기사 제목 (편의상 넣음 꼭 필요한 것은 아님)\n",
    "        topic = page.select('div > ul > li > dl > dt')[1].text.strip() \n",
    "\n",
    "        # link 딕셔너리에 헤드라인 서브 페이지 링크, 관련 기사 건수, 첫번째 기사 제목  \n",
    "        link[idx] = {'head_link':each_url, '관련기사':news_num, 'topic': topic} \n",
    "\n",
    "        # 헤드라인 서브 페이지 별로 언론사 리스트 추출\n",
    "        np_list = page.select('span.writing')\n",
    "        counter = {}\n",
    "        for np_name in np_list:\n",
    "            if np_name.text in ['조선일보', '동아일보', '경향신문', '한겨레', '한국일보', '중앙일보']:\n",
    "                if np_name.text not in counter:\n",
    "                    counter[np_name.text] = 0\n",
    "                counter[np_name.text] += 1\n",
    "            else:\n",
    "                pass\n",
    "        total[idx] = counter # key: index / value : 언론사 별 기사 \n",
    "\n",
    "    df1 = pd.DataFrame.from_dict(link, orient = 'index') # 링크, 관련기사 수, topic 있는 dataframe\n",
    "    df2 = pd.DataFrame.from_dict(total, orient = 'index') # 각 언론사별 기사 갯수 있는 dataframe\n",
    "\n",
    "    # merge\n",
    "    df3 = pd.merge(df1.reset_index(), df2.reset_index(), on = 'index', how = 'outer').drop('index', axis = 1)\n",
    "\n",
    "    global con\n",
    "    global pro\n",
    "    con = ['조선일보', '동아일보']\n",
    "    pro = ['경향신문', '한겨레']\n",
    "    \n",
    "\n",
    "    for name in con+pro:\n",
    "        df3[f'{name}_presence'] = df3[f'{name}'].apply(lambda x :1 if x>0 else 0)\n",
    "    df3['보수'] = df3['조선일보_presence'] + df3['동아일보_presence'] # 보수신문 2개 있는지\n",
    "    df3['진보'] = df3['경향신문_presence'] + df3['한겨레_presence'] # 진보신문 2개 있는지\n",
    "    df3['합계'] = df3['보수'] + df3['진보']\n",
    "    df3 = df3.drop(['조선일보_presence','동아일보_presence','경향신문_presence','한겨레_presence'], axis=1)\n",
    "    \n",
    "    return df3\n",
    "\n",
    "def choice_title(url,topic):\n",
    "    df3 = main_page(url,topic)\n",
    "    \n",
    "    global max_idx\n",
    "    global selected_url\n",
    "    \n",
    "    if df3['합계'].argmax() == df3['관련기사'].argmax():\n",
    "        max_idx = df3['합계'].argmax()\n",
    "        selected_url = df3['head_link'][max_idx]\n",
    "    elif len(df3[(df3['진보'] > 0) & (df3['보수'] > 0)]) > 0:\n",
    "        max_idx = df3['관련기사'].argmax()\n",
    "        selected_url = df3['head_link'][max_idx]\n",
    "    else:\n",
    "        max_idx = df3['합계'].argmax()\n",
    "        selected_url = df3['head_link'][max_idx]\n",
    "\n",
    "\n",
    "    global new_con\n",
    "    global new_pro\n",
    "    \n",
    "    if df3['합계'][max_idx] == 4:\n",
    "        print('관련기사 건수', df3['관련기사'].max())\n",
    "        print('기사 4개, 선택된 url', selected_url)\n",
    "\n",
    "        # 기사 4개 일때 언론사는 원래 그대로 \n",
    "        new_con = con\n",
    "        new_pro = pro\n",
    "        print('선택한 언론사', new_pro + new_con)\n",
    "\n",
    "    # 보수, 진보 합쳐서 3개 있는 df - 관련기사 가장 많은 헤드라인의 링크\n",
    "    elif df3['합계'][max_idx] == 3:\n",
    "        print('관련기사 건수', df3['관련기사'].max())\n",
    "        print('기사 3개, 선택된 url', selected_url)\n",
    "        # 여기에 포함 안된 언론사\n",
    "        absent_name = df3.loc[max_idx][pro+con].notna().idxmin()\n",
    "        print('제외된 언론사', absent_name)\n",
    "\n",
    "        # 기사 3개 일때 선택할 언론사\n",
    "        if absent_name in pro:\n",
    "            new_pro = list(set(pro) - {absent_name}) + ['한국일보']\n",
    "            new_con = con\n",
    "        else:\n",
    "            new_pro = pro\n",
    "            new_con = list(set(con) - {absent_name}) + ['중앙일보']\n",
    "        print('선택한 언론사', new_pro + new_con)\n",
    "\n",
    "    # 보수, 진보 각각 1개 있는 df - 관련기사 가장 많은 헤드라인의 링크\n",
    "    elif (df3['합계'][max_idx] < 3) &( df3['합계'][max_idx]>0):\n",
    "        print('관련기사 건수', df3['관련기사'].max())\n",
    "        print('기사 2개, 선택된 url', selected_url)\n",
    "        # 여기에 포함 안된 언론사\n",
    "        absent_name_con = df3.loc[max_idx][con].notna().idxmin()\n",
    "        absent_name_pro = df3.loc[max_idx][pro].notna().idxmin()\n",
    "        print('제외된 언론사', absent_name_con, absent_name_pro)\n",
    "\n",
    "        # 보수, 진보 각각 1개씩 있을 때 선택할 언론사\n",
    "        new_pro = list(set(pro) - {absent_name_pro}) + ['한국일보']\n",
    "        new_con = list(set(con) - {absent_name_con}) + ['중앙일보']\n",
    "        print('선택한 언론사', new_pro + new_con)\n",
    "\n",
    "    return selected_url\n",
    "# 선택된 헤드라인 페이지에서 신문사별로 링크 따오기\n",
    "def choice_link(url,topic):\n",
    "    \n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup as bs\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from datetime import datetime\n",
    "    \n",
    "    selected_url = choice_title(url,topic)\n",
    "    req = requests.get(selected_url, headers = headers)\n",
    "    target = bs(req.content, 'html.parser')\n",
    "\n",
    "    con_link1 = {} # 보수 언론사 1\n",
    "    con_link2 = {} # 보수 언론사 2\n",
    "    pro_link1 = {} # 진보 언론사 1\n",
    "    pro_link2 = {} # 진보 언론사 2\n",
    "    con_num1 = 0 \n",
    "    con_num2 = 0\n",
    "    pro_num1 = 0\n",
    "    pro_num2 = 0\n",
    "\n",
    "    for tag in target.select('div > ul > li > dl'):\n",
    "        if tag.select_one('dd > span.writing').text == new_con[0]: # 보수 언론사 1 이름과 동일할 경우\n",
    "            con_num1 += 1 # 기사 번호\n",
    "            con_link1[con_num1] = tag.select_one('dt > a')['href'] # 기사 번호 당 링크\n",
    "        if tag.select_one('dd > span.writing').text == new_con[1]: # 보수 언론사 2 이름과 동일할 경우\n",
    "            con_num2 += 1 # 기사 번호\n",
    "            con_link2[con_num2] = tag.select_one('dt > a')['href'] # 기사 번호 당 링크\n",
    "        if tag.select_one('dd > span.writing').text == new_pro[0]: # 진보 언론사 1 이름과 동일할 경우\n",
    "            pro_num1 += 1 # 기사 번호\n",
    "            pro_link1[pro_num1] = tag.select_one('dt > a')['href'] # 기사 번호 당 링크\n",
    "        if tag.select_one('dd > span.writing').text == new_pro[1]: # 진보 언론사 2 이름과 동일할 경우\n",
    "            pro_num2 += 1 # 기사 번호\n",
    "            pro_link2[pro_num2] = tag.select_one('dt > a')['href'] # 기사 번호 당 링크\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    total_links = {new_con[0]: con_link1, new_con[1]: con_link2, new_pro[0]: pro_link1, new_pro[1]: pro_link2}\n",
    "        \n",
    "    def compare_time(links):\n",
    "        article_times = {} \n",
    "        for key, url in links.items():\n",
    "            req = requests.get(url, headers = headers)\n",
    "            target = bs(req.content, 'html.parser')\n",
    "            time = target.select('span.t11')[-1].text # 기사 입력 시간 추출 (최초 작성 후 수정본이 있을때 수정된 시간으로 추출 / 만약 최초 작성 시간 기준을 하고 싶으면 인덱스[0]) \n",
    "            time = time.replace('오후', 'PM').replace('오전', 'AM') # 오후 -> PM, 오전 -> AM 변경\n",
    "            time = datetime.strptime(time, '%Y.%m.%d. %p %I:%M') # datetime 으로 파싱\n",
    "            article_times[key] = time # key: 원래 딕셔너리의 key 입력, value: 기사 입력 시간\n",
    "        \n",
    "        selected_article = links[max(article_times, key = article_times.get)] # article_times 딕셔너리의 value가 max인(가장 최근인) key로 con_link1 딕셔너리의 value(링크)찾기\n",
    "        return selected_article\n",
    "\n",
    "    final_links = {}\n",
    "    for name, link in total_links.items():\n",
    "        try:\n",
    "            final_links[name] = compare_time(link)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return final_links\n",
    "  \n",
    "def make_df(topic):\n",
    "    \n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup as bs\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from datetime import datetime\n",
    "    \n",
    "    global url\n",
    "    if topic == '경제':\n",
    "        url = 'https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=101'\n",
    "    elif topic == '정치':\n",
    "        url = 'https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=100'\n",
    "    elif topic == '사회':\n",
    "        url = 'https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=102'\n",
    "\n",
    "    final_links = choice_link(url,topic)\n",
    "    # 정치면 기사 dataframe\n",
    "    time = []\n",
    "    media = []\n",
    "    head = []\n",
    "    body = []\n",
    "\n",
    "    for name, link in final_links.items():\n",
    "        req = requests.get(link, headers=headers)\n",
    "        soup = bs(req.content, 'html.parser')\n",
    "        time.append(soup.select_one('span.t11').text) # time\n",
    "        media.append(soup.select_one('div.article_header > div.press_logo > a > img')['title']) # media\n",
    "        title = soup.find(id= 'articleTitle')\n",
    "        try:\n",
    "            head.append(title.text)\n",
    "            text = soup.find(id = 'articleBodyContents')\n",
    "            body.append(text.text.split('_flash_removeCallback() {}\\n\\n')[-1])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    politics_df = pd.DataFrame({'time':time,'media':media,'title':head,'document':body})\n",
    "    return politics_df\n",
    "\n",
    "# url 지정\n",
    "make_df('경제')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
