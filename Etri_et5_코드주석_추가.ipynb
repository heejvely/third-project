{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Etri_et5_코드주석 추가.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heejvely/third-project/blob/main/Etri_et5_%EC%BD%94%EB%93%9C%EC%A3%BC%EC%84%9D_%EC%B6%94%EA%B0%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23RcYKGZFSro"
      },
      "source": [
        "참고 url : https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR-heIHg7zJ4"
      },
      "source": [
        "# 버전에 맞는 module 설치\n",
        "!pip install transformers\n",
        "!pip install sentencepiece==0.1.91"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYBZdQIKQ4Vl"
      },
      "source": [
        "transformers version = '4.12.5'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPfEMbfCEo5u"
      },
      "source": [
        "런타임 다시 시작 눌러야할 수 있음  \n",
        "만일 tokenizer Nonetype 에러시 런타임 다시 시작"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V43hOR_o8t8Y"
      },
      "source": [
        "from transformers import T5Config, T5Tokenizer, T5ForConditionalGeneration\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 모델 패스 지정\n",
        "model_folder = './etri_et5'\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_folder)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e34lHWGtsSWd"
      },
      "source": [
        "# gpu 환경 설정\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeNA2dpdtAVi"
      },
      "source": [
        "# model에 넣기 위한 형식으로 dataset 변경\n",
        "class CustomDataset:\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_len = source_len\n",
        "        self.summ_len = summ_len\n",
        "        self.text = self.data.text\n",
        "        self.ctext = self.data.ctext\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        ctext = str(self.ctext[index])\n",
        "        ctext = ' '.join(ctext.split())\n",
        "\n",
        "        text = str(self.text[index])\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n",
        "        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n",
        "\n",
        "        # batch_encode_plus: 인코딩된 sequences 쌍의 추가 정보를 포함하여 사전의 index 반환\n",
        "        \"\"\"\n",
        "        [tokenizer.batch_encode_plus]\n",
        "        encode_plus => single,   batch_encode_plus => a list of pairs of sequences\n",
        "        \n",
        "        < example >\n",
        "            input_text = ['첫번째 문장', '두번째 문장']\n",
        "            \n",
        "            [input]\n",
        "                tokenizer.encode_plus(input_text)\n",
        "            [output]\n",
        "                {'input_ids': [2, 2, 1], 'attention_mask': [1, 1, 1]}\n",
        "\n",
        "            [input]\n",
        "                tokenizer.batch_encode_plus(input_text)\n",
        "            [output]\n",
        "                {'input_ids': [[21437, 19862, 1], [17521, 19862, 1]], 'attention_mask': [[1, 1, 1], [1, 1, 1]]}\n",
        "\n",
        "           \"\"\"\n",
        "\n",
        "\n",
        "        # input_ids: 문장을 토크나이즈해서 인덱스값으로 변환\n",
        "        # attention_mask: 패딩된 부분에 대해 학습에 영향을 받지 않기 위해 처리해주는 입력값\n",
        "        source_ids = source['input_ids'].squeeze()\n",
        "        source_mask = source['attention_mask'].squeeze()\n",
        "        target_ids = target['input_ids'].squeeze()\n",
        "        target_mask = target['attention_mask'].squeeze()\n",
        "        # Tensor.sqeeze() => 차원이 1인 차원 제거\n",
        "\n",
        "        return {\n",
        "            'source_ids': source_ids.to(dtype=torch.long), \n",
        "            'source_mask': source_mask.to(dtype=torch.long), \n",
        "            'target_ids': target_ids.to(dtype=torch.long),\n",
        "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
        "            # torch.long => 64비트의 부호있는 정수(64-bit integer (signed))\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcvmRVxasS_d"
      },
      "source": [
        "# model Fine tuning를 위한 사용자 함수\n",
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "    model.train()\n",
        "    for _,data in tqdm(enumerate(loader, 0)):\n",
        "        y = data['target_ids'].to(device, dtype = torch.long)\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        lm_labels = y[:, 1:].clone().detach()\n",
        "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
        "        loss = outputs[0]\n",
        "        \n",
        "        if _%10 == 0:\n",
        "            pass\n",
        "            \n",
        "        if _%500==0:\n",
        "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        \"\"\"\n",
        "        gradient를 0으로 설정\n",
        "        이 작업을 하지 않을 시 학습중 backward를 해줄 때 계속 반영되어 예기치않은 방향으로 학습할 수 있음\n",
        "        \n",
        "        \"\"\"\n",
        "        loss.backward()  # 역전파 단계\n",
        "        optimizer.step() # update hyper-parameters to optimizer\n",
        "       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww9SpZIhsS8O"
      },
      "source": [
        "# downsteam model이 test 진행을 하기 위한 사용자 함수\n",
        "def validate(epoch, tokenizer, model, device, loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(loader, 0):\n",
        "            y = data['target_ids'].to(device, dtype = torch.long)\n",
        "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "            # train과 다르게 generate할 수 있는 코드가 포함됨.\n",
        "            generated_ids = model.generate(\n",
        "                input_ids = ids,\n",
        "                attention_mask = mask, \n",
        "                max_length=150, \n",
        "                num_beams=2,\n",
        "                repetition_penalty=2.5, \n",
        "                length_penalty=1.0, \n",
        "                early_stopping=True\n",
        "                )\n",
        "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
        "            if _%100==0:\n",
        "                print(f'Completed {_}')\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            actuals.extend(target)\n",
        "    return predictions, actuals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li1ZjzuosS5o"
      },
      "source": [
        "# gpu 환경 적용\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPz-4_59r3Jt"
      },
      "source": [
        "hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbU1ySuJsS36"
      },
      "source": [
        "# hyper parameter 설정\n",
        "config = T5Config()\n",
        "config.MAX_LEN = 1024\n",
        "config.SUMMARY_LEN = 150 \n",
        "config.TRAIN_BATCH_SIZE = 2    # input batch size for training (default: 64)\n",
        "config.VALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)\n",
        "config.TRAIN_EPOCHS = 3        # number of epochs to train (default: 10)\n",
        "config.VAL_EPOCHS = 1 \n",
        "config.LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n",
        "config.SEED = 42               # random seed (default: 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hLshDtNsS1R"
      },
      "source": [
        "# hyper parameter train, valid에 적용\n",
        "train_params = {\n",
        "        'batch_size': config.TRAIN_BATCH_SIZE,\n",
        "        'shuffle': True,\n",
        "        'num_workers': 0\n",
        "        }\n",
        "\n",
        "val_params = {\n",
        "        'batch_size': config.VALID_BATCH_SIZE,\n",
        "        'shuffle': False,\n",
        "        'num_workers': 0\n",
        "        }\n",
        "\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=config.LEARNING_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrru0dvtxaWA"
      },
      "source": [
        "# 현재 사용하고 있는 GPU의 스펙과 작동 상황\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z5DClk92JFV"
      },
      "source": [
        "# train, validation dataset 설정\n",
        "import pandas as pd\n",
        "train_dataset = pd.read_csv('/content/drive/MyDrive/3차 프로젝트/dataset/train.csv')[['document','label']]\n",
        "validation_dataset = pd.read_csv('/content/drive/MyDrive/3차 프로젝트/dataset/valid.csv')[['document','label']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4JFTHoisIGQ"
      },
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "U7e4LNJssSzF"
      },
      "source": [
        "# downsteam train 진행\n",
        "# 모델이 학습하기 위한 columns name 변경 (ctext: 원문, text: 요약문)\n",
        "train_dataset.columns = ['ctext','text']\n",
        "\n",
        "# t5는 prefix 모델로써, 진행할 task를 입력할 시 해당 task 진행\n",
        "# summarize 진행 할 예정으로 summarize 추가\n",
        "train_dataset.ctext = 'summarize: ' + train_dataset.ctext\n",
        "\n",
        "# 위에서 정의한 model에 맞는 dataset 형식 변경하는 함수 적용\n",
        "training_set = CustomDataset(train_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
        "\n",
        "# DataLoader : 일일히 모든 데이터를 나눠서 forward, backward 작업을 해야하는데 그 과정을 대신 해주는 과정\n",
        "# 학습 시 minibatch, epoch마다 데이터를 다시 섞어 과적합을 막음. \n",
        "training_loader = DataLoader(training_set, **train_params) # **var -> {key:value} 형식으로 반환\n",
        "\n",
        "\n",
        "for epoch in range(config.TRAIN_EPOCHS):\n",
        "    print (1)\n",
        "    train(epoch, tokenizer, model, device, training_loader, optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2OKiEwzsKTs"
      },
      "source": [
        "valid(test)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0ly0z2xsSwq"
      },
      "source": [
        "validation_dataset.columns = ['ctext','text']\n",
        "validation_dataset.ctext = 'summarize: ' + validation_dataset.ctext\n",
        "\n",
        "val_set = CustomDataset(validation_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
        "\n",
        "val_loader = DataLoader(val_set, **val_params)\n",
        "\n",
        "for epoch in range(config.VAL_EPOCHS):\n",
        "    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
        "    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
        "\n",
        "final_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgE-LRHAzhOR"
      },
      "source": [
        "# model, tokenizer 저장\n",
        "tokenizer.save_pretrained('./pretrained/')\n",
        "model.save_pretrained('./pretrained/')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}